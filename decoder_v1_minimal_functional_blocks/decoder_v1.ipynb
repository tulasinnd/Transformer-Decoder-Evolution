{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6d0c30",
   "metadata": {},
   "source": [
    "### **Decoder: Autoregressive Next-Token Prediction**\n",
    "\n",
    "* This is a minimal decoder-only Transformer built to understand how information flows through a decoder during next-token prediction.\n",
    "* The model takes a sequence of token representations, applies causal self-attention and feed-forward transformations, and produces vocabulary logits for\n",
    "each position in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37dd5df",
   "metadata": {},
   "source": [
    "#### **Notation**\n",
    "\n",
    "- **B** — batch size  \n",
    "- **T** — sequence length (time steps)  \n",
    "- **D** — model dimension (`d_model`)  \n",
    "- **V** — vocabulary size  \n",
    "\n",
    "All tensors follow the shape convention:\n",
    "\n",
    "* (B, T, D) — for token representations  \n",
    "* (B, T, V) — for output logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Imports\n",
    "# ------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# ------------------------\n",
    "# Initial configuration\n",
    "# ------------------------\n",
    "vocab_size = 20   # V \n",
    "d_model = 4       # D (token vector dimention)\n",
    "seq_len = 6       # T (number of tokens in input sentence)\n",
    "batch_size = 1    # B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002c7b5",
   "metadata": {},
   "source": [
    "#### **1 Single-Head Causal Self-Attention**\n",
    "\n",
    "This section implements the **core operation of a decoder-only Transformer**: causal self-attention.The goal here is not efficiency or multi-head scaling, but to make the attention mechanism **fully explicit and inspectable**\n",
    "\n",
    "- query, key, value projections\n",
    "- causal masking\n",
    "- attention weight computation\n",
    "- context aggregation\n",
    "\n",
    "This attention module operates on a full sequence and enforces **autoregressive (left-to-right) information flow**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c99bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN  : token representations (B, T, D)\n",
    "# OUT : context-aware representations (B, T, D) and attention weights for inspection (B, T, T)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.Wq_projection = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk_projection = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wv_projection = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # Output projection\n",
    "        self.Wo_projection = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, S, D = X.shape\n",
    "\n",
    "        # Project input to queries, keys, values\n",
    "        Q = self.Wq_projection(X)\n",
    "        K = self.Wk_projection(X)\n",
    "        V = self.Wv_projection(X)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "\n",
    "        # Causal mask to prevent access to future tokens\n",
    "        causal_mask = torch.tril(torch.ones(S, S))\n",
    "        scores = scores.masked_fill(causal_mask == 0, -1e9)\n",
    "\n",
    "        # Attention weights\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        # Final projection\n",
    "        output = self.Wo_projection(output)\n",
    "\n",
    "        return output, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960c203",
   "metadata": {},
   "source": [
    "### **2 Layer Normalization**\n",
    "\n",
    "- Layer Normalization stabilizes training by **normalizing each token’s feature vector independently**.\n",
    "- This keeps activations well-behaved, improves gradient flow, and makes deep Transformer training stable.\n",
    "- This implementation uses PyTorch’s built-in `LayerNorm` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1b3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN  : token representations (B, T, D)\n",
    "# OUT : normalized representations (B, T, D)\n",
    "\n",
    "class LN(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.norm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32555e",
   "metadata": {},
   "source": [
    "### **3 Feed-Forward Network**\n",
    "\n",
    "* The Feed-Forward Network applies a **position-wise nonlinearity** to each token independently.\n",
    "* It expands the model dimension, applies a non-linear transformation, and projects back to the original dimension.\n",
    "* This allows the decoder to learn **nonlinear feature interactions** after attention has mixed information across tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13d3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN  : (B, T, D)\n",
    "# OUT : (B, T, D)\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, 4 * d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4 * d_model, d_model)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.relu(self.linear1(X))\n",
    "        out = self.linear2(h)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462694a",
   "metadata": {},
   "source": [
    "### **4 Output Projection (Logits)**\n",
    "\n",
    "* The output projection maps each token’s final representation to vocabulary-sized logits.\n",
    "* These logits are later used for next-token prediction via a softmax and cross-entropy loss during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3914eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN  : (B, T, D)\n",
    "# OUT : (B, T, V)\n",
    "\n",
    "class Logit(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491ec9c",
   "metadata": {},
   "source": [
    "### **5 Decoder Block**\n",
    "\n",
    "* This section wires together all components into a single decoder layer using residual connections and layer normalization.\n",
    "* The decoder processes a sequence autoregressively and produces vocabulary logits for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "514d735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN  : (B, T, D)\n",
    "# OUT : (B, T, V)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = Attention(d_model)\n",
    "        self.ln1 = LN(d_model)\n",
    "\n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ln2 = LN(d_model)\n",
    "\n",
    "        self.logit = Logit(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        attn_out, weights = self.attn(X)\n",
    "        X = self.ln1(X + attn_out)\n",
    "\n",
    "        ffn_out = self.ffn(X)\n",
    "        X = self.ln2(X + ffn_out)\n",
    "\n",
    "        logits = self.logit(X)\n",
    "        return logits, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6ab8b",
   "metadata": {},
   "source": [
    "### **6 Single-Step Training Sanity Check**\n",
    "\n",
    "* This section performs a minimal forward and backward pass to verify that the decoder is fully differentiable and can learn from data.\n",
    "* The purpose here is **not convergence**, but to validate end-to-end data flow, loss computation, and gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede580f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Model & Optimizer\n",
    "# ------------------------\n",
    "model = Decoder(vocab_size, d_model)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# ------------------------\n",
    "# Dummy Batch (Sanity Check)\n",
    "# ------------------------\n",
    "X = torch.randn(batch_size, seq_len, d_model)                         # input representations for each token\n",
    "Y = torch.randint(0, vocab_size, (batch_size, seq_len))               # the target token IDs the model should predict\n",
    "\n",
    "# ------------------------\n",
    "# Forward\n",
    "# ------------------------\n",
    "logits, weights = model(X)   # (B, T, V)\n",
    "\n",
    "B, T, V = logits.shape\n",
    "logits = logits.view(B * T, V)\n",
    "Y = Y.view(B * T)\n",
    "\n",
    "# ------------------------\n",
    "# Loss\n",
    "# ------------------------\n",
    "loss = loss_fn(logits, Y)\n",
    "\n",
    "# ------------------------\n",
    "# Backward\n",
    "# ------------------------\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825617d5",
   "metadata": {},
   "source": [
    "### **Important Observations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb80065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X shape: torch.Size([1, 6, 4])\n",
      "\n",
      "Output logits shape: torch.Size([6, 20])\n",
      "\n",
      "Attention weights shape: torch.Size([1, 6, 6])\n",
      "\n",
      "Attention weights (batch 0):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5292, 0.4708, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4307, 0.4072, 0.1621, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1702, 0.2278, 0.3822, 0.2199, 0.0000, 0.0000],\n",
      "        [0.1436, 0.1866, 0.2123, 0.2014, 0.2561, 0.0000],\n",
      "        [0.2025, 0.1726, 0.1301, 0.1739, 0.1413, 0.1797]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "Logits at first position:\n",
      "tensor([-0.6833,  0.2061,  0.3898,  0.3143, -0.5294, -0.1016,  0.1618,  0.4335,\n",
      "         0.7510,  1.6260, -0.8837,  0.2265,  1.1763, -0.6008,  0.9962,  0.8220,\n",
      "         0.1334, -0.5884, -0.3020,  0.5791], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Loss: 3.23246693611145\n",
      "\n",
      "Grad norm (Wq): tensor(0.0190)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 1. Input / Output Shape Validation\n",
    "# --------------------------------------------------\n",
    "# Ensures that tensor dimensions align across the model.\n",
    "# If shapes are correct, the forward pass is structurally sound.\n",
    "\n",
    "print(\"Input X shape:\", X.shape)           # (B, T, D)\n",
    "print(\"\\nOutput logits shape:\", logits.shape) # (B, T, V)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Attention Weight Inspection\n",
    "# --------------------------------------------------\n",
    "# The attention matrix reveals autoregressive behavior.\n",
    "# Each token should attend only to itself and previous tokens,\n",
    "# confirming that causal masking is applied correctly.\n",
    "\n",
    "print(\"\\nAttention weights shape:\", weights.shape)  # (B, T, T)\n",
    "print(\"\\nAttention weights (batch 0):\")\n",
    "print(weights[0])\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Logit Inspection\n",
    "# --------------------------------------------------\n",
    "# Logits represent unnormalized scores over the vocabulary.\n",
    "# At initialization, these scores are random, leading to an\n",
    "# approximately uniform probability distribution.\n",
    "\n",
    "print(\"\\nLogits at first position:\")\n",
    "print(logits[0])\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Loss Inspection\n",
    "# --------------------------------------------------\n",
    "# With random initialization, the loss should be approximately log(V),\n",
    "# indicating uniform guessing across the vocabulary.\n",
    "\n",
    "print(\"\\nLoss:\", loss.item())\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Gradient Flow Check\n",
    "# --------------------------------------------------\n",
    "# A non-zero gradient at the query projection (Wq) confirms that\n",
    "# learning signals propagate from the loss back through the entire model.\n",
    "\n",
    "print(\n",
    "    \"\\nGrad norm (Wq):\",\n",
    "    model.attn.Wq_projection.weight.grad.norm()\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
